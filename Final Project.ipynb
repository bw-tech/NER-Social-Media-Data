{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append word into current sentence\n",
    "# if this word is a new line, append current sentence ot all sentences and then refresh current sent\n",
    "def load_data(data_path):\n",
    "    with open(data_path) as data:\n",
    "        sentences = []\n",
    "        tags = []\n",
    "        curr_sen = []\n",
    "        curr_tag = []\n",
    "        i = 0\n",
    "        for phrase in data:\n",
    "            if phrase != '\\n':\n",
    "                word, tag = phrase.split('\\t')\n",
    "                curr_sen.append(word)\n",
    "                # strip removes the trailing '\\n'\n",
    "                curr_tag.append(tag.strip())\n",
    "            if phrase == '\\n':\n",
    "                sentences.append(curr_sen)\n",
    "                tags.append(curr_tag)\n",
    "                curr_sen = []\n",
    "                curr_tag = []\n",
    "                \n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(data_path):\n",
    "    with open(data_path) as data:\n",
    "        sentences = []\n",
    "        test_data = []\n",
    "        for phrase in data.read().strip().split('\\n\\n'):\n",
    "            phrase = phrase.strip()\n",
    "            lines = phrase.split('\\n')\n",
    "            test_data.append([line.strip() for line in lines])\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'data/train/train.txt'\n",
    "sentences, tags = load_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "1. remove hashtags\n",
    "2. mentions\n",
    "3. change punctuation to a unique tag\n",
    "4. changes links to link tag\n",
    "5. change verbs to verb tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_dicts(corpus):\n",
    "    vocab_counts = {}\n",
    "    vocab_ids = {}\n",
    "    id_num = 1\n",
    "    vocab_counts['-UNK-'] = 0\n",
    "    vocab_ids['-UNK-'] = 0\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(len(corpus[i])):\n",
    "            curr_word = corpus[i][j]\n",
    "            \n",
    "            # make dict with key = vocab, value = id\n",
    "            if curr_word not in vocab_ids:\n",
    "                vocab_ids[curr_word] = id_num\n",
    "                id_num += 1\n",
    "                \n",
    "            # make dict with key = vocab, value = freq\n",
    "            if curr_word not in vocab_counts:\n",
    "                vocab_counts[curr_word] = 1\n",
    "            else:\n",
    "                vocab_counts[curr_word] += 1\n",
    "                \n",
    "    return vocab_counts, vocab_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counts, vocab_ids = create_vocab_dicts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, vocab_count_dict, word_threshold):\n",
    "    \n",
    "    num_hashtags = 0\n",
    "    num_mentions = 0\n",
    "    num_puncs = 0\n",
    "    num_links = 0\n",
    "    num_rare = 0\n",
    "    num_digit = 0\n",
    "    special_tags = ['-HASHTAG-', '-MENTION-', '-PUNC-', '-LINK-']\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(len(corpus[i])):\n",
    "            curr_word = corpus[i][j]\n",
    "        \n",
    "            if curr_word[0] == '#':\n",
    "                corpus[i][j] = '-HASHTAG-'\n",
    "                num_hashtags += 1\n",
    "                \n",
    "            if curr_word[0] == '@':\n",
    "                corpus[i][j] = '-MENTION-'\n",
    "                num_mentions += 1\n",
    "            \n",
    "            if curr_word[0] in '!$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~':\n",
    "                corpus[i][j] = '-PUNC-'\n",
    "                num_puncs += 1\n",
    "            \n",
    "            if (curr_word[0:4] == 'http') or (curr_word[0:3] == 'www'):\n",
    "                corpus[i][j] = '-LINK-'\n",
    "                num_links += 1\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(len(corpus[i])):\n",
    "            curr_word = corpus[i][j]\n",
    "            \n",
    "            if curr_word in special_tags:\n",
    "                continue\n",
    "                \n",
    "            if curr_word.isdigit():\n",
    "                corpus[i][j] = '-DIGIT-'\n",
    "                num_digit += 1\n",
    "                \n",
    "#             if vocab_count_dict[curr_word] <= word_threshold:\n",
    "#                 corpus[i][j] = '-RARE-'\n",
    "#                 num_rare += 1\n",
    "                    \n",
    "    vocab_counts, vocab_ids = create_vocab_dicts(corpus)\n",
    "\n",
    "    print('Total Changed Words')\n",
    "    print('-------------------')\n",
    "    print('Hashtags:      {}'.format(num_hashtags))\n",
    "    print('Mentions:      {}'.format(num_mentions))\n",
    "    print('Punctuation:   {}'.format(num_puncs))\n",
    "    print('Links:         {}'.format(num_links))\n",
    "#     print('Rare:          {}'.format(num_rare))\n",
    "    print('Digit:         {}'.format(num_digit))\n",
    "    print('-------------------')\n",
    "    print('Vocab Size:    {}'.format(len(vocab_counts)))\n",
    "    \n",
    "    return corpus, vocab_counts, vocab_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocab_counts, vocab_ids = preprocess(sentences, vocab_counts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_ids(corpus, vocab2ID_dict):\n",
    "    \n",
    "    num_unk = 0\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(len(corpus[i])):\n",
    "            try:\n",
    "                corpus[i][j] = vocab_ids[corpus[i][j]]\n",
    "            except:\n",
    "                corpus[i][j] = vocab_ids['-UNK-']\n",
    "                num_unk += 1\n",
    "    \n",
    "    print('Number of Unknown Words: {}'.format(num_unk))\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = words_to_ids(corpus, vocab_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_vectors(tags):\n",
    "    for i in range(len(tags)):\n",
    "        for j in range(len(tags[i])):\n",
    "            curr_tag = tags[i][j]\n",
    "            if curr_tag == 'B':\n",
    "                tags[i][j] = [1, 0, 0]\n",
    "            elif curr_tag == 'I':\n",
    "                tags[i][j] = [0, 1, 0]\n",
    "            elif curr_tag == 'O':\n",
    "                tags[i][j] = [0, 0, 1]\n",
    "            else:\n",
    "                print('bro what')\n",
    "                break\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_onehot(tags):\n",
    "    for i in range(len(tags)):\n",
    "        for j in range(len(tags[i])):\n",
    "            curr_tag = tags[i][j]\n",
    "            if curr_tag == 'B':\n",
    "                tags[i][j] = 2\n",
    "            elif curr_tag == 'I':\n",
    "                tags[i][j] = 1\n",
    "            elif curr_tag == 'O':\n",
    "                tags[i][j] = 0\n",
    "            else:\n",
    "                print('Invalid tag!')\n",
    "                break\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tags_to_onehot(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_path = 'data/dev/dev.txt'\n",
    "dev_data, dev_tags = load_data(dev_path)\n",
    "\n",
    "dev_vocab_counts, dev_vocab_ids = create_vocab_dicts(dev_data)\n",
    "dev_corpus, dev_vocab_counts, dev_vocab_ids = preprocess(dev_data, dev_vocab_counts, 1)\n",
    "\n",
    "dev_corpus = words_to_ids(dev_corpus, vocab_ids)\n",
    "\n",
    "dev_tags = tags_to_onehot(dev_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/test/test.nolabels.txt'\n",
    "test_data = load_test_data(test_path)\n",
    "\n",
    "test_vocab_counts, test_vocab_ids = create_vocab_dicts(test_data)\n",
    "test_corpus, test_vocab_counts, test_vocab_ids = preprocess(test_data, test_vocab_counts, 1)\n",
    "\n",
    "test_corpus = words_to_ids(test_corpus, vocab_ids)\n",
    "\n",
    "# test_tags = tags_to_onehot(test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_epochs, learning_rate, momentum, \n",
    "                 vocab_size, embedding_dim, lstm_dim1, lstm_dim2, lstm_dim3):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.num_epochs = num_epochs\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_hidden_dim1 = lstm_dim1\n",
    "        self.lstm_hidden_dim2 = lstm_dim2\n",
    "        self.lstm_hidden_dim3 = lstm_dim3\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(self.embedding_dim, self.lstm_hidden_dim1)\n",
    "        self.lstm2 = nn.LSTM(self.lstm_hidden_dim1, self.lstm_hidden_dim2)\n",
    "        self.lstm3 = nn.LSTM(self.lstm_hidden_dim2, self.lstm_hidden_dim3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_dim3, int(self.lstm_hidden_dim3/2))\n",
    "        self.fc2 = nn.Linear(int(self.lstm_hidden_dim2/2), 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = x.view(1, -1, x.shape[1])\n",
    "        \n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        \n",
    "        x = x.contiguous()\n",
    "        x = x.view(-1, x.shape[2])\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim = 1)\n",
    "        #return x\n",
    "    \n",
    "    def train_and_evaluate(self, data, labels, dev_data, dev_labels):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        total_training_loss = []\n",
    "        total_dev_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(net.parameters(), lr = self.learning_rate, momentum = self.momentum)\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_start = time.time()\n",
    "\n",
    "            print('Starting Epoch: {}'.format(epoch))\n",
    "            print('------------------------')\n",
    "            train_loss = 0\n",
    "\n",
    "            for i, (sentence, label) in enumerate(zip(data, labels)):            \n",
    "                sentence, label = torch.LongTensor(sentence), torch.LongTensor(label)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = net(sentence)\n",
    "                loss = criterion(outputs, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()    \n",
    "                train_loss += loss.item()\n",
    "\n",
    "            total_training_loss.append(train_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                running_dev_loss = 0\n",
    "                for i, (sentencedev, labeldev) in enumerate(zip(dev_data, dev_labels)):\n",
    "                    sentencedev, labeldev = torch.LongTensor(sentencedev), torch.LongTensor(labeldev)\n",
    "\n",
    "                    net.eval()\n",
    "\n",
    "                    prediction = net(sentencedev)\n",
    "\n",
    "                    dev_loss = criterion(prediction, labeldev)\n",
    "                    running_dev_loss += dev_loss.item()\n",
    "\n",
    "            total_dev_loss.append(dev_loss)\n",
    "\n",
    "            epoch_end = time.time()\n",
    "            total_epoch_time = round((epoch_end - epoch_start) / 60, 2)\n",
    " \n",
    "            print('Epoch: {}\\nTraining Loss: {}\\nDev Loss: {}'.format(epoch, train_loss, dev_loss))\n",
    "            print('Epoch run time: {} minutes\\n'.format(total_epoch_time))\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = round((end_time - start_time) / 60, 2)\n",
    "\n",
    "        print('Finished training in {} minutes'.format(total_time))\n",
    "        return total_training_loss, total_dev_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('', len(corpus), len(corpus[0]), '\\n', len(tags), len(tags[0]))\n",
    "print('', len(dev_corpus), len(dev_corpus[0]), '\\n', len(dev_tags), len(dev_tags[0]))\n",
    "\n",
    "print('', len(vocab_counts))\n",
    "print('', len(dev_vocab_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "vocab_size = len(vocab_ids)\n",
    "embedding_dim = 1000\n",
    "lstm_dim1 = 500\n",
    "lstm_dim2 = 200\n",
    "lstm_dim3 = 100\n",
    "\n",
    "# net = Net()\n",
    "\n",
    "net = Net(num_epochs, learning_rate, momentum, vocab_size, embedding_dim, lstm_dim1, lstm_dim2, lstm_dim3)\n",
    "\n",
    "total_train_loss, total_dev_loss = net.train_edev(corpus, tags, dev_corpus, dev_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Net Architecture:')\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_transform(network, data, tag_index_dict):\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in data:\n",
    "            sample = torch.LongTensor(sample)\n",
    "            predicted_label = network(sample)\n",
    "            predictions.append(predicted_label)\n",
    "\n",
    "    trans_pred = []\n",
    "    for prediction in predictions:\n",
    "        curr_sample = []\n",
    "        for label in prediction:\n",
    "            label = np.argsort(-label.numpy())[0]\n",
    "            trans_label = tag_index_dict[label]\n",
    "            curr_sample.append(trans_label)\n",
    "        trans_pred.append(curr_sample)\n",
    "    \n",
    "    return trans_pred\n",
    "\n",
    "def export_predictions(file_path, predictions):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for sample in predictions:\n",
    "            for label in sample:\n",
    "                line = label + '\\n'\n",
    "                f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys\n",
    "\n",
    "def warning(msg):\n",
    "    print(\"WARNING:\", msg)\n",
    "\n",
    "def convert_bio_to_spans(bio_sequence):\n",
    "    spans = []  # (label, startindex, endindex)\n",
    "    cur_start = None\n",
    "    cur_label = None\n",
    "    N = len(bio_sequence)\n",
    "    for t in range(N+1):\n",
    "        if ((cur_start is not None) and\n",
    "                (t==N or re.search(\"^[BO]\", bio_sequence[t]))):\n",
    "            assert cur_label is not None\n",
    "            spans.append((cur_label, cur_start, t))\n",
    "            cur_start = None\n",
    "            cur_label = None\n",
    "        if t==N: continue\n",
    "        assert bio_sequence[t] and bio_sequence[t][0] in (\"B\",\"I\",\"O\")\n",
    "        if bio_sequence[t].startswith(\"B\"):\n",
    "            cur_start = t\n",
    "            cur_label = re.sub(\"^B-?\",\"\", bio_sequence[t]).strip()\n",
    "        if bio_sequence[t].startswith(\"I\"):\n",
    "            if cur_start is None:\n",
    "               #warning(\"BIO inconsistency: I without starting B. Rewriting to B.\")\n",
    "                newseq = bio_sequence[:]\n",
    "                newseq[t] = \"B\" + newseq[t][1:]\n",
    "                return convert_bio_to_spans(newseq)\n",
    "            continuation_label = re.sub(\"^I-?\",\"\",bio_sequence[t])\n",
    "            if continuation_label != cur_label:\n",
    "                newseq = bio_sequence[:]\n",
    "                newseq[t] = \"B\" + newseq[t][1:]\n",
    "                #warning(\"BIO inconsistency: %s but current label is '%s'. Rewriting to %s\" % (bio_sequence[t], cur_label, newseq[t]))\n",
    "                return convert_bio_to_spans(newseq)\n",
    "\n",
    "    # should have exited for last span ending at end by now\n",
    "    assert cur_start is None\n",
    "    spancheck(spans)\n",
    "    return spans\n",
    "\n",
    "def test_bio_conversion():\n",
    "    spans = convert_bio_to_spans([\"B\"])\n",
    "    assert spans==[(\"\",0,1)]\n",
    "    spans = convert_bio_to_spans([\"B\",\"I\"])\n",
    "    assert spans==[(\"\",0,2)]\n",
    "    spans = convert_bio_to_spans([\"B\",\"I\",\"O\"])\n",
    "    assert spans==[(\"\",0,2)]\n",
    "    spans = convert_bio_to_spans([\"O\",\"B\",\"I\",\"O\",\"O\"])\n",
    "    assert spans==[(\"\",1,3)]\n",
    "    spans = convert_bio_to_spans([\"B\",\"B\"])\n",
    "    assert spans==[(\"\",0,1), (\"\",1,2)]\n",
    "    spans = convert_bio_to_spans([\"B\",\"I\",\"B\"])\n",
    "    assert spans==[(\"\",0,2), (\"\",2,3)]\n",
    "    spans = convert_bio_to_spans([\"B-asdf\",\"I-asdf\",\"B\"])\n",
    "    assert spans==[(\"asdf\",0,2), (\"\",2,3)]\n",
    "    spans = convert_bio_to_spans([\"B-asdf\",\"I-difftype\",\"B\"])\n",
    "    assert spans==[(\"asdf\",0,1), (\"difftype\",1,2), (\"\",2,3)]\n",
    "    spans = convert_bio_to_spans([\"I\",\"I\"])\n",
    "    assert spans==[(\"\",0,2)]\n",
    "    spans = convert_bio_to_spans([\"B-a\",\"I-b\"])\n",
    "    assert spans==[(\"a\",0,1), (\"b\",1,2)]\n",
    "\n",
    "\n",
    "def spancheck(spanlist):\n",
    "    s = set(spanlist)\n",
    "    assert len(s)==len(spanlist), \"spans are non-unique ... is this a bug in the eval script?\"\n",
    "\n",
    "def kill_labels(bio_seq):\n",
    "    ret = []\n",
    "    for x in bio_seq:\n",
    "        if re.search(\"^[BI]\", x):\n",
    "            x = re.sub(\"^B.*\",\"B\", x)\n",
    "            x = re.sub(\"^I.*\",\"I\", x)\n",
    "        ret.append(x)\n",
    "    return ret\n",
    "\n",
    "def evaluate_taggings(goldseq_predseq_pairs, ignore_labels=False):\n",
    "    \"\"\"a list of (goldtags,predtags) pairs.  goldtags and predtags are both lists of strings, of the same length.\"\"\"\n",
    "    num_sent = 0\n",
    "    num_tokens= 0\n",
    "    num_goldspans = 0\n",
    "    num_predspans = 0\n",
    "\n",
    "    tp, fp, fn = 0,0,0\n",
    "\n",
    "    for goldseq,predseq in goldseq_predseq_pairs:\n",
    "        N = len(goldseq)\n",
    "        assert N==len(predseq)\n",
    "        num_sent += 1\n",
    "        num_tokens += N\n",
    "\n",
    "        if ignore_labels:\n",
    "            goldseq = kill_labels(goldseq)\n",
    "            predseq = kill_labels(predseq)\n",
    "\n",
    "        goldspans = convert_bio_to_spans(goldseq)\n",
    "        predspans = convert_bio_to_spans(predseq)\n",
    "\n",
    "        num_goldspans += len(goldspans)\n",
    "        num_predspans += len(predspans)\n",
    "\n",
    "        goldspans_set = set(goldspans)\n",
    "        predspans_set = set(predspans)\n",
    "\n",
    "        tp += len(goldspans_set & predspans_set)\n",
    "        fp += len(predspans_set - goldspans_set)\n",
    "        fn += len(goldspans_set - predspans_set)\n",
    "\n",
    "    prec = tp/(tp+fp) if (tp+fp)>0 else 0\n",
    "    rec =  tp/(tp+fn) if (tp+fn)>0 else 0\n",
    "    f1 = 2*prec*rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "    print(\"F = {f1:.4f},  Prec = {prec:.4f} ({tp}/{tpfp}),  Rec = {rec:.4f} ({tp}/{tpfn})\".format(\n",
    "            tpfp=tp+fp, tpfn=tp+fn, **locals()))\n",
    "    print(\"({num_sent} sentences, {num_tokens} tokens, {num_goldspans} gold spans, {num_predspans} predicted spans)\".format(**locals()))\n",
    "\n",
    "def read_tokens_tags_file(filename):\n",
    "    \"\"\"Returns list of sentences.  each sentence is a pair (tokens, tags), each\n",
    "    of which is a list of strings of the same length.\"\"\"\n",
    "    sentences = open(filename).read().strip().split(\"\\n\\n\")\n",
    "    ret = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        lines = sent.split(\"\\n\")\n",
    "        pairs = [L.split(\"\\t\") for L in lines]\n",
    "        for pair in pairs:\n",
    "            assert len(pair)==2, \"Was expecting 2 tab-separated items per line.\"\n",
    "        tokens = [tok for tok,tag in pairs]\n",
    "        tags = [tag for tok,tag in pairs]\n",
    "        ret.append( (tokens,tags) )\n",
    "    return ret\n",
    "\n",
    "def read_tags_file(filename):\n",
    "    sentences = open(filename).read().strip().split(\"\\n\\n\")\n",
    "    ret = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        lines = sent.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            assert len(line.split())==1, \"Was expecting 1 item per line\"\n",
    "        ret.append( [line.strip() for line in lines] )\n",
    "    return ret\n",
    "\n",
    "def evaluate_tagging_file(gold_tags_file, predicted_tags_file):\n",
    "\n",
    "    tokens_and_tags = read_tokens_tags_file(gold_tags_file)\n",
    "    goldseqs = [tags for tokens,tags in tokens_and_tags]\n",
    "    predtags = read_tags_file(predicted_tags_file)\n",
    "\n",
    "    print(\"Span-level NER evaluation\")\n",
    "    evaluate_taggings( list(zip(goldseqs, predtags)), ignore_labels=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred = predict(dev_data)\n",
    "\n",
    "tag_index_dict = {2: 'B', 1: 'I', 0: 'O'}\n",
    "trans_dev_pred = transform_predictions(dev_pred, tag_index_dict)\n",
    "\n",
    "pred_file_path = 'results/dev/dev_pred.out'\n",
    "export_predictions(pred_file_path, trans_dev_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_tagging_file('data/dev/dev.txt', 'results/dev_preds.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_tagging_file('data/dev/dev.txt', 'results/dev/dev_epoch_4_true.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index_dict = {2: 'B', 1: 'I', 0: 'O'}\n",
    "dev_pred = predict_and_transform(net, dev_data, tag_index_dict)\n",
    "\n",
    "pred_file_path = 'results/dev/dev_pred.out'\n",
    "export_predictions(pred_file_path, dev_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_tagging_file('data/dev/dev.txt', 'results/dev/Preds.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"B\", \"I\", \"O\"]\n",
    "tag_to_idx = {t: i for i, t in enumerate(classes)}\n",
    "tag_to_idx['[PAD]'] = -100\n",
    "idx_to_tag = {i: t for t, i in tag_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': 0, 'I': 1, 'O': 2, '[PAD]': -100}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B', 1: 'I', 2: 'O', -100: '[PAD]'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
